# Mini-GPT (PyTorch)

A **from-scratch implementation of a GPT-style language model** using PyTorch.

This project is intentionally minimal and educational â€” designed to help you **deeply understand Transformers, attention, tokenization, training loops, and text generation**, without relying on high-level libraries like HuggingFace.

---

## ğŸš€ Features

- Character-level tokenizer (fully custom)
- GPT-style Transformer architecture
- Causal self-attention
- Feed-forward networks with expansion
- Training + inference pipeline
- Text generation (autoregressive)
- Model checkpoint saving/loading

---

## ğŸ“‚ Project Structure

```
mini-gpt/
â”‚
â”œâ”€â”€ main.py                  # Entry point (train + generate)
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ gpt.py               # MiniGPT model
â”‚   â”œâ”€â”€ transformer_block.py # Attention + FFN block
â”‚   â””â”€â”€ attention.py         # Causal self-attention
â”‚
â”œâ”€â”€ tokenizer/
â”‚   â””â”€â”€ char_tokenizer.py    # Character-level tokenizer
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py             # Training loop
â”‚   â”œâ”€â”€ dataset.py           # Batch sampling
â”‚   â””â”€â”€ config.py            # Hyperparameters
â”‚
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ generate.py          # Text generation logic
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ input.txt            # Training text corpus
â”‚
â””â”€â”€ checkpoints/
    â””â”€â”€ model.pt             # Saved model weights
```

---

## ğŸ“¦ Installation

Create and activate a virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸ§  Training the Model

Add your training text to:

```
data/input.txt
```

Then run:

```bash
python main.py
```

The model will:
- Train from scratch **or** load saved weights if available
- Save the model to `checkpoints/model.pt`

---

## âœ¨ Text Generation

After training completes, the model automatically generates text.

You can control:
- Starting token / prompt
- Number of generated tokens

Generation is **autoregressive**, one token at a time.

---

## âš™ï¸ Configuration

Edit hyperparameters in:

```
training/config.py
```

Example:

```python
config = {
    "batch_size": 32,
    "block_size": 128,
    "d_model": 128,
    "n_heads": 4,
    "n_layers": 4,
    "lr": 3e-4,
    "max_iters": 3000
}
```

---

## ğŸ“– Learning Goals

This project is designed to help you understand:

- How GPT-style models work internally
- Why attention + feed-forward layers are both necessary
- How tokenization affects learning
- How autoregressive generation works
- How model capacity influences language quality

---

## ğŸ§ª Why Character-Level Tokenization?

- Extremely simple
- Transparent
- Perfect for learning

Later, this can be extended to:
- BPE / WordPiece
- SentencePiece
- Byte-level encoding

---

## ğŸ”® Future Improvements

- Add GELU instead of ReLU
- Add dropout
- Add validation split
- Add temperature / top-k sampling
- Add CLI arguments
- Switch to subword tokenization

---

## ğŸ§‘â€ğŸ’» Author

Built as a **learning-focused GPT implementation** by a Computer Science student exploring deep learning and language models.

---

## ğŸ“œ License

MIT License â€” free to use, modify, and learn from.
